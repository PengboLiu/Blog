---
title: '[机器学习笔记] SVM学习笔记（一）'
date: 2018-12-08 23:58:12
categories: 
  - 机器学习
  - SVM
tags:
  - 机器学习
  - SVM
 
---



# SVM学习笔记
本文是《统计学习方法》上“支持向量机”一节的读书笔记。一些是书中重要内容摘抄，一些是笔者的理解，还有一些来自于其他机器学习资料（如林轩田《机器学习技法》）。  

**支持向量机**（Support Vector Machine，SVM）是一种二分类模型。基本模型是定义在特征空间上的间隔最大的分类器。  
支持向量机包括很多种模型：线性可分支持向量机、线性支持向量机及非线性支持向量机。   

一个二分类问题，假设输入空间和特征空间为两个不同的空间。输入空间为欧式空间或离散集合，特征空间为欧式空间或希尔伯特空间。**线性可分支持向量机**、**线性支持向量机**假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量；**非线性支持向量机**利用非线性映射将输入转化到特征空间中（这也就是为什么叫它“非线性”）。注意：**支持向量机的学习是在特征空间进行的**。  
## 线性可分支持向量机与硬间隔最大化 ##
假设给定一个在特征空间上的训练数据集
$$T = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \cdots , \left( x _ { N } , y _ { N } \right) \right\}$$
其中$x _ { i } \in \mathcal { X } = \mathbf { R } ^ { n } , \quad y _ { i } \in \mathcal { Y } = \{ + 1 , - 1 \} , \quad i = 1,2 , \cdots , N$，$x_i$为第$i$个特征向量，$y_i$为$x_i$的类标记，当$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例,$(x_i,y_i)$称为样本点。我们假设数据是线性可分的。
线性可分支持向量机的学习目标是在特征空间中找到一个分类超平面，将实例分类。超平面的方程为$w \cdot x + b = 0$，$x$是特征向量，$w$（法向量）和$b$（截距）是需要我们求解的参数，方程也可以用$(w,b)$表示。法向量指向的一侧为正类，另一侧是负类。  
一般来说，存在着无数个超平面将数据正确分开，但我们通过**间隔最大化**可以求得唯一的最优分离超平面。

### 线性可分支持向量机 ###
给定线性可分训练数据集，通过间隔最大法得到的分离超平面$w ^ { * } \cdot x + b ^ { * } = 0$，以及分类决策函数$f ( x ) = \operatorname { sign } \left( w ^ { * } \cdot x + b ^ { * } \right)$ 称之为“**线性可分支持向量机**”。
如下图，有很多条直线可以将圈和叉正确地分开，但是线性可分支持向量机只对应着能正确分开并且间隔最大的那条直线。 
![avatar](/images/2018-12-08-svm-01.png)
那么，如何能得到这条唯一的直线呢？首先我们要知道上面说的“间隔”是什么。

### 函数间隔和几何间隔 ###
#### 函数间隔 ####
对于数据集 $T$ 和超平面 $(w,b)$，我们定义超平面 $(w,b)$关于每个样本点$(x_i,y_i)$的函数间隔为：
$$\hat { \gamma } _ { i } = y _ { i } \left( w \cdot x _ { i } + b \right)$$
我们定义超平面 $(w,b)$关于整个数据集 $T$ 的函数间隔为超平面 $(w,b)$关于所有样本点函数间隔的最小值：
$$\hat { \gamma } = \min _ { i = 1 , \cdots , N } \hat { \gamma } _ { i }$$
但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$（如将它们改成2$w$和2$b$），则函数间隔的值$f(x)$却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够，我们需要几何间隔。
#### 几何间隔 ####
对于数据集 $T$ 和超平面 $(w,b)$，我们定义超平面 $(w,b)$关于每个样本点$(x_i,y_i)$的几何间隔为：
$$\gamma _ { i } = y _ { i } \left( \frac { w } { \| w \| } \cdot x _ { i } + \frac { b } { \| w \| } \right)$$
其中$\| w \|$为$w$的二阶范数（范数是一个类似于模的表示长度的概念），$\frac { w } { \| w \| }$是单位向量（一个向量除以它的模称之为单位向量）。
我们定义超平面 $(w,b)$关于整个数据集 $T$ 的几何间隔为超平面 $(w,b)$关于所有样本点几何间隔的最小值：
$$ { \gamma } = \min _ { i = 1 , \cdots , N }  { \gamma } _ { i }$$
#### 函数间隔与几何间隔的关系 ####
由定义可知，函数间隔$\hat\gamma$与几何间隔$\gamma$有如下关系：
$$\begin{aligned} \gamma _ { i } & = \frac { \hat { \gamma } _ { i } } { \| \boldsymbol { w } \| } \\ \gamma & = \frac { \hat { \gamma } } { \| \boldsymbol { w } \| } \end{aligned}$$
其实，几何间隔就是函数间隔除以$\| w \|$，而且函数间隔 $y _ { i } \left( w \cdot x _ { i } + b \right)=y \cdot f(x)$  实际上就是$\|f(x)\|$，只是人为定义的一个间隔度量，而几何间隔$\frac { f ( x ) } { \| w \| }$才是直观上的点到超平面的距离。
#### 最大间隔化 ####
直观上来讲，对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。注意，这里说的间隔指的是“**几何间隔**”。
那么我们如何求一个几何间隔最大的超平面呢？首先，我们可以用数学语言描述一下这个问题：
$$\max _ { w , b } \gamma$$
$$s.t. \quad y _ { i } \left( \frac { w } { \| w \| } \cdot x _ { i } + \frac { b } { \| w \| } \right) \geqslant \gamma , \quad i = 1,2 , \cdots , N$$
第一行表示我们希望几何间隔能够最大（即分类的确信度尽可能大），第二行表示超平面关于每个训练样本点的几何间隔至少是$\gamma$（即每个点都要被正确地分类）。
